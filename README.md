# Language-Model
Abstractâ€” In case of language modeling, both word-level and character-level models were trained on the same neural network architecture and the model perplexity was plotted against the number of epochs. The network architecture comprises of an embedding layer dimension of 300, two LSTM layers with 300 units each followed by a dense layer with softmax activation. The hyperparameters chosen and few other observations are discussed in detail and associated results have been visualized through various plots. The perplexity on the validation set after 50 epochs of training came out to be 182.6 for the word-level model and 2.98 for the character-level model.
