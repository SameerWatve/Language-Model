# Language-Model
Abstractâ€”The following report details our design and imple- mentation of Recurrent Neural Network using LSTM units to demonstrate the power of LSTMs in applications such as se- quence counting and language modeling. For sequence counting, a single LSTM unit was manually configured with different parameters to count the digit 0 in a sequence under different criteria. In case of language modeling, both word-level and character-level models were trained on the same neural network architecture and the model perplexity was plotted against the number of epochs. The network architecture comprises of an embedding layer dimension of 300, two LSTM layers with 300 units each followed by a dense layer with softmax activation. The hyperparameters chosen and few other observations are discussed in detail and associated results have been visualized through various plots. The perplexity on the validation set after 50 epochs of training came out to be 182.6 for the word-level model and 2.98 for the character-level model.
